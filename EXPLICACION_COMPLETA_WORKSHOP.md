# ğŸ“š ExplicaciÃ³n Completa del Workshop MLflow + Docker

## ğŸ¯ Objetivo del Workshop

Este workshop implementÃ³ el **ciclo completo de vida de un modelo de Machine Learning** usando MLflow y Docker, desde la exploraciÃ³n inicial de datos hasta el despliegue como servicio REST. El enfoque fue **prÃ¡ctico y conceptual**, priorizando el entendimiento del flujo sobre la complejidad del modelo.

---

## ğŸ—ï¸ Arquitectura Implementada

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WORKSHOP ARCHITECTURE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  LOCAL MACHINE                    DOCKER CONTAINER         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Data Explorationâ”‚              â”‚   MLflow Tracking  â”‚   â”‚
â”‚  â”‚ - explore_data.pyâ”‚              â”‚   - train.py       â”‚   â”‚
â”‚  â”‚ - Pandas        â”‚              â”‚   - Experiments    â”‚   â”‚
â”‚  â”‚ - Basic EDA     â”‚              â”‚   - Metrics        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚   - Model Registry â”‚   â”‚
â”‚           â”‚                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                                â”‚                â”‚
â”‚           â–¼                                â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Dataset       â”‚              â”‚   MLflow UI        â”‚   â”‚
â”‚  â”‚ - iris.csv      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚   - Port 5000      â”‚   â”‚
â”‚  â”‚ - 150 samples   â”‚              â”‚   - Experiments    â”‚   â”‚
â”‚  â”‚ - 4 features    â”‚              â”‚   - Model Registry â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                             â”‚                â”‚
â”‚                                             â–¼                â”‚
â”‚                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚                                    â”‚   Model Serving    â”‚   â”‚
â”‚                                    â”‚   - REST API       â”‚   â”‚
â”‚                                    â”‚   - /invocations   â”‚   â”‚
â”‚                                    â”‚   - Port 1235      â”‚   â”‚
â”‚                                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Fases Implementadas Paso a Paso

### ğŸ” FASE 1: ExploraciÃ³n Local de Datos

**Archivo**: `src/explore_data.py`

**Objetivo**: Entender los datos antes de entrenar cualquier modelo.

**QuÃ© se hizo**:
```python
# Cargar dataset Iris
iris = datasets.load_iris()
X, y = iris.data, iris.target

# AnÃ¡lisis exploratorio
- Shape: (150, 6) - 150 muestras, 4 features + target
- Tipos: 4 features numÃ©ricas (float64)
- Target: 3 clases balanceadas (50 cada una)
- Calidad: Sin valores faltantes
- DistribuciÃ³n: Datos ya normalizados
```

**Conceptos aplicados**:
- **Data Lifecycle**: IngestiÃ³n â†’ Calidad â†’ TransformaciÃ³n
- **Criterio de datos**: ClasificaciÃ³n multiclase, dataset limpio
- **Split mental**: Features vs Target, 80/20 train/test

**Resultado**: Dataset guardado en `data/iris_dataset.csv` listo para entrenamiento.

---

### ğŸ³ FASE 2: Entorno Reproducible con Docker

**Archivo**: `docker/Dockerfile`

**Objetivo**: Crear un entorno aislado y reproducible para el entrenamiento.

**QuÃ© se hizo**:
```dockerfile
FROM python:3.10-slim          # Base ligera
WORKDIR /app                   # Directorio de trabajo
COPY requirements.txt .        # Dependencias primero (cache)
RUN pip install -r requirements.txt
COPY src/ src/                 # CÃ³digo fuente
COPY data/ data/               # Dataset
EXPOSE 5000                    # Puerto MLflow UI
CMD ["python", "src/train.py"] # Comando por defecto
```

**Conceptos aplicados**:
- **Reproducibilidad**: Mismo entorno en cualquier mÃ¡quina
- **Aislamiento**: Dependencias controladas
- **OptimizaciÃ³n**: Layer caching de Docker
- **Principio**: Menos dependencias = menos problemas

**Resultado**: Imagen Docker `mlflow-train` construida exitosamente.

---

### ğŸ¯ FASE 3: Entrenamiento + MLflow Tracking

**Archivo**: `src/train.py`

**Objetivo**: Entrenar modelo Y registrar todo el proceso en MLflow.

**QuÃ© se hizo**:

#### 3.1 PreparaciÃ³n de Datos
```python
# Split estratificado
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
# Train: 120 muestras, Test: 30 muestras
```

#### 3.2 ConfiguraciÃ³n MLflow
```python
mlflow.set_experiment("MLflow Workshop - Iris Classification")
```

#### 3.3 Entrenamiento con Tracking
```python
with mlflow.start_run() as run:
    # 1. Registrar hiperparÃ¡metros
    mlflow.log_param("solver", "lbfgs")
    mlflow.log_param("max_iter", 1000)
    mlflow.log_param("C", 1.0)
    
    # 2. Entrenar modelo
    model = LogisticRegression(**params)
    model.fit(X_train, y_train)
    
    # 3. Calcular mÃ©tricas
    test_accuracy = accuracy_score(y_test, y_pred_test)
    # ... mÃ¡s mÃ©tricas
    
    # 4. Registrar mÃ©tricas
    mlflow.log_metric("test_accuracy", test_accuracy)
    mlflow.log_metric("test_f1", test_f1)
    
    # 5. Registrar modelo
    mlflow.sklearn.log_model(
        sk_model=model,
        artifact_path="iris_classifier",
        registered_model_name="iris_logistic_regression"
    )
```

**Resultados obtenidos**:
- **Test Accuracy**: 96.67% (excelente)
- **Overfitting**: 0.83% (muy bajo)
- **F1-Score**: 96.66% (balanceado)
- **Run ID**: `57986499592a46f58d4ded4b54f06bc3`

**Conceptos aplicados**:
- **MLflow Run**: Experimento individual con ID Ãºnico
- **Parameters**: HiperparÃ¡metros del modelo
- **Metrics**: MÃ©tricas de evaluaciÃ³n
- **Artifacts**: Modelo serializado + metadatos
- **Experiment**: AgrupaciÃ³n de runs relacionados

---

### ğŸ·ï¸ FASE 4: Model Registry

**Interfaz**: MLflow UI (http://localhost:5000)

**Objetivo**: Versionar y gestionar modelos como artefactos de software.

**QuÃ© se hizo**:

#### 4.1 MLflow UI Iniciado
```bash
mlflow ui --host 0.0.0.0 --port 5000
```

#### 4.2 ExploraciÃ³n en la UI
- **Experiments Tab**: Ver runs, mÃ©tricas, parÃ¡metros
- **Models Tab**: Ver modelos registrados
- **Model Registry**: GestiÃ³n de versiones y stages

#### 4.3 Conceptos Clave Demostrados

**Diferencia fundamental**:
```
âŒ Archivo .pkl = Solo pesos del modelo
âœ… Modelo registrado = CÃ³digo + Pesos + Entorno + MÃ©tricas + Lineage
```

**Stages del modelo**:
- **None**: ReciÃ©n registrado
- **Staging**: Listo para testing
- **Production**: Sirviendo usuarios reales
- **Archived**: Deprecado

**Versionado automÃ¡tico**:
- Cada registro crea nueva versiÃ³n
- Trazabilidad completa
- Rollback posible

---

### ğŸŒ FASE 5: Model Serving

**Archivos**: `src/serve_model.py`, `src/predict_test.py`

**Objetivo**: Servir modelo como API REST consumible por otros sistemas.

**QuÃ© se implementÃ³**:

#### 5.1 Servidor Flask Personalizado
```python
@app.route('/invocations', methods=['POST'])
def predict():
    # Input: {"instances": [[5.1, 3.5, 1.4, 0.2]]}
    # Output: {"predictions": [0]}
```

#### 5.2 Endpoints Implementados
- `GET /health`: Health check del servicio
- `POST /invocations`: Predicciones (formato MLflow)
- `POST /predict_names`: Predicciones con nombres de clases
- `GET /`: InformaciÃ³n del servicio

#### 5.3 Test de Inferencia
```python
# Datos de prueba
test_samples = [
    [5.1, 3.5, 1.4, 0.2],  # Setosa
    [7.0, 3.2, 4.7, 1.4],  # Versicolor  
    [6.3, 3.3, 6.0, 2.5]   # Virginica
]

# Request format
data = {"instances": test_samples}
response = requests.post(url, json=data)
```

**Conceptos aplicados**:
- **Modelo como servicio**: API REST estÃ¡ndar
- **Formato MLflow**: Compatible con ecosistema
- **Escalabilidad**: MÃºltiples requests concurrentes
- **IntegraciÃ³n**: Consumible por otros sistemas

---

## ğŸ”„ ConexiÃ³n Data Lifecycle â†’ Model Lifecycle

### Mapeo Conceptual Implementado

| **Data Lifecycle** | **Model Lifecycle** | **QuÃ© se Mantiene** | **QuÃ© se AmplÃ­a** |
|-------------------|-------------------|-------------------|------------------|
| **IngestiÃ³n** | Feature Extraction | Pipelines de datos | Versionado de features |
| **TransformaciÃ³n** | Training | Reproducibilidad | MÃ©tricas de performance |
| **Consumo** | Inference | APIs y outputs | Serving y monitoreo |

### Flujo Completo Implementado

```
Datos Raw â†’ ExploraciÃ³n â†’ Features â†’ Entrenamiento â†’ Modelo â†’ Registro â†’ Serving â†’ Predicciones
    â†“           â†“           â†“           â†“           â†“         â†“         â†“          â†“
  CSV       Pandas      Arrays     Scikit-learn  MLflow   Registry   Flask    JSON API
```

---

## ğŸ› ï¸ TecnologÃ­as y Herramientas Utilizadas

### Stack TÃ©cnico
- **Python 3.10**: Lenguaje base
- **Pandas**: ManipulaciÃ³n de datos
- **Scikit-learn**: Machine Learning
- **MLflow**: Tracking y registry
- **Docker**: ContainerizaciÃ³n
- **Flask**: Web serving
- **SQLite**: Base de datos MLflow

### Dependencias MÃ­nimas
```txt
pandas>=1.5.0
scikit-learn>=1.3.0
mlflow>=2.8.0
flask>=3.1.0
```

---

## ğŸ“Š Resultados y MÃ©tricas Obtenidas

### Performance del Modelo
```
âœ… Test Accuracy: 96.67%
âœ… Train Accuracy: 97.50%
âœ… Overfitting: 0.83% (excelente)
âœ… F1-Score: 96.66%
âœ… Precision: 96.67%
âœ… Recall: 96.67%
```

### MÃ©tricas del Proceso
```
âœ… Tiempo total: ~2 horas
âœ… Fases completadas: 5/5
âœ… Experimentos trackeados: 1
âœ… Modelos registrados: 1
âœ… Versiones: 1
âœ… APIs implementadas: 4 endpoints
```

---

## ğŸš€ Escalabilidad hacia ProducciÃ³n

### Mapeo Taller â†’ ProducciÃ³n Real

| **Componente Taller** | **Equivalente ProducciÃ³n** | **Servicio AWS** |
|----------------------|---------------------------|------------------|
| Docker local | Container Registry | Amazon ECR |
| MLflow local | Managed ML Platform | Amazon SageMaker |
| Serving Flask | API Gateway + Compute | API Gateway + Lambda |
| Manual deployment | CI/CD Pipelines | CodePipeline + CodeBuild |
| SQLite tracking | Managed database | RDS + S3 |
| Local experiments | Distributed training | SageMaker Training Jobs |

### PrÃ³ximos Pasos TÃ©cnicos
1. **CI/CD**: Automatizar entrenamiento y despliegue
2. **Monitoring**: MÃ©tricas de modelo en producciÃ³n
3. **A/B Testing**: Comparar versiones de modelos
4. **Auto-scaling**: Manejar carga variable
5. **Security**: AutenticaciÃ³n y autorizaciÃ³n

---

## ğŸ’¡ Conceptos Clave Aprendidos

### 1. MLflow = Git + Docker + MÃ©tricas para Modelos
```
Git:     Versionado de cÃ³digo
Docker:  Entornos reproducibles  
MLflow:  Versionado de modelos + mÃ©tricas + tracking
```

### 2. Modelo â‰  Archivo
```
âŒ modelo.pkl = Solo pesos
âœ… Modelo MLflow = CÃ³digo + Pesos + Entorno + MÃ©tricas + Lineage
```

### 3. Principios de MLOps Aplicados
- **Reproducibilidad**: Mismo resultado en cualquier entorno
- **Trazabilidad**: Saber cÃ³mo se creÃ³ cada modelo
- **Versionado**: Control de cambios en modelos
- **AutomatizaciÃ³n**: Reducir intervenciÃ³n manual
- **Monitoreo**: Observabilidad en producciÃ³n

### 4. SeparaciÃ³n de Responsabilidades
```
Local:   ExploraciÃ³n y desarrollo
Docker:  Entrenamiento y tracking  
MLflow:  Registry y governance
APIs:    Serving y consumo
```

---

## ğŸ¯ Mensajes Clave del Workshop

> ### "Un modelo que no estÃ¡ trackeado, no existe"
> Sin tracking, no hay reproducibilidad, comparaciÃ³n ni governance.

> ### "Un modelo que no se puede consumir, es solo un experimento"
> El valor estÃ¡ en la capacidad de generar predicciones para sistemas reales.

> ### "MLflow no es solo para ML, Docker no es solo DevOps"
> Es software engineering aplicado a modelos de Machine Learning.

> ### "El modelo no importa, el flujo sÃ­ importa"
> La infraestructura y procesos son mÃ¡s crÃ­ticos que el algoritmo especÃ­fico.

---

## ğŸ“š Archivos Generados y Su PropÃ³sito

### Estructura Final del Proyecto
```
project/
â”œâ”€â”€ README.md                          # DocumentaciÃ³n principal
â”œâ”€â”€ requirements.txt                   # Dependencias mÃ­nimas
â”œâ”€â”€ EXPLICACION_COMPLETA_WORKSHOP.md   # Este documento
â”œâ”€â”€ WORKSHOP_COMPLETED.md              # Resumen de logros
â”œâ”€â”€ PHASE_4_INSTRUCTIONS.md            # GuÃ­a MLflow UI
â”œâ”€â”€ data/
â”‚   â””â”€â”€ iris_dataset.csv              # Dataset procesado
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ explore_data.py               # Fase 1: ExploraciÃ³n
â”‚   â”œâ”€â”€ train.py                      # Fase 3: Entrenamiento
â”‚   â”œâ”€â”€ serve_model.py                # Fase 5: Serving
â”‚   â””â”€â”€ predict_test.py               # Fase 5: Testing
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile                    # Fase 2: ContainerizaciÃ³n
â””â”€â”€ mlruns/                           # MLflow artifacts (generado)
    â””â”€â”€ [experimentos y modelos]
```

### PropÃ³sito de Cada Archivo

**ExploraciÃ³n**:
- `explore_data.py`: EDA y preparaciÃ³n inicial
- `iris_dataset.csv`: Dataset limpio para entrenamiento

**Entrenamiento**:
- `train.py`: Pipeline completo con MLflow tracking
- `Dockerfile`: Entorno reproducible
- `requirements.txt`: Dependencias controladas

**Serving**:
- `serve_model.py`: API REST personalizada
- `predict_test.py`: Tests de inferencia

**DocumentaciÃ³n**:
- `README.md`: GuÃ­a de uso
- `PHASE_4_INSTRUCTIONS.md`: Instrucciones MLflow UI
- `WORKSHOP_COMPLETED.md`: Resumen de logros
- `EXPLICACION_COMPLETA_WORKSHOP.md`: DocumentaciÃ³n tÃ©cnica completa

---

## ğŸ† Conclusiones y Logros

### âœ… Objetivos Cumplidos
1. **Flujo completo implementado**: Datos â†’ Modelo â†’ Serving
2. **MLflow mastery**: Tracking, Registry, UI
3. **Docker proficiency**: Entornos reproducibles
4. **API development**: Serving como servicio
5. **MLOps foundations**: Principios y mejores prÃ¡cticas

### ğŸ§  Conocimientos Adquiridos
- **Versionado de modelos** como artefactos de software
- **Tracking de experimentos** para reproducibilidad
- **ContainerizaciÃ³n** para consistencia de entornos
- **APIs REST** para serving de modelos
- **Governance** de modelos en equipos

### ğŸš€ Capacidades Desarrolladas
- Implementar pipelines de ML end-to-end
- Usar MLflow para gestiÃ³n de modelos
- Containerizar aplicaciones de ML
- Servir modelos como APIs
- Documentar y versionar experimentos

---

## ğŸ”® PrÃ³ximos Pasos Recomendados

### Inmediatos (1-2 semanas)
1. **Experimentar** con diferentes algoritmos en el mismo pipeline
2. **Explorar MLflow UI** mÃ¡s profundamente
3. **Probar** con datasets propios
4. **Implementar** mÃ¡s mÃ©tricas y visualizaciones

### Mediano Plazo (1-3 meses)
1. **Estudiar** las referencias complementarias
2. **Implementar** pipelines automatizados
3. **Explorar** SageMaker y servicios cloud
4. **Practicar** con problemas mÃ¡s complejos

### Largo Plazo (3-6 meses)
1. **DiseÃ±ar** arquitecturas MLOps completas
2. **Implementar** monitoreo de modelos en producciÃ³n
3. **Desarrollar** expertise en servicios cloud especÃ­ficos
4. **Contribuir** a proyectos open source de MLOps

---

**ğŸ‰ Â¡Felicitaciones por completar exitosamente el workshop "MLflow + Docker: del dato al modelo desplegado"!**

*Ya tienes las bases sÃ³lidas para construir sistemas de Machine Learning robustos y escalables.*