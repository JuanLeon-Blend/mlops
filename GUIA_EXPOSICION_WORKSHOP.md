# üé§ Gu√≠a de Exposici√≥n: Workshop MLflow + Docker

## üìã Estructura de la Presentaci√≥n (45-60 minutos)

---

## üéØ INTRODUCCI√ìN (5 minutos)

### Slide 1: T√≠tulo y Contexto
**"MLflow + Docker: Del Dato al Modelo Desplegado"**

**Qu√© vas a decir:**
> "Hoy vamos a ver c√≥mo implementamos un pipeline completo de Machine Learning, desde la exploraci√≥n de datos hasta tener un modelo sirviendo predicciones via API REST. No se trata de hacer el mejor modelo, sino de entender c√≥mo los modelos pueden vivir en el mundo real."

### Slide 2: El Problema que Resolvemos
**Mostrar estos pain points:**
- "¬øQu√© modelo entren√© la semana pasada?"
- "¬øCon qu√© par√°metros funcion√≥ mejor?"
- "¬øC√≥mo pongo este modelo en producci√≥n?"
- "¬øC√≥mo s√© si mi modelo sigue funcionando?"

**Qu√© vas a decir:**
> "Estos son problemas reales que enfrentamos cuando pasamos de notebooks a sistemas productivos. MLflow y Docker nos ayudan a resolverlos."

### Slide 3: Arquitectura Mental
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Local Machine    ‚îÇ  ‚Üê Solo exploraci√≥n
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Docker        ‚îÇ  ‚Üê Training, tracking, serving
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇ    MLflow     ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Qu√© vas a decir:**
> "La regla de oro: Python local solo para explorar datos. Todo lo importante ocurre en Docker. Esto nos da reproducibilidad desde el d√≠a uno."

---

## üîç FASE 1: EXPLORACI√ìN DE DATOS (8 minutos)

### Slide 4: ¬øPor qu√© Empezar con Datos?
**Qu√© vas a decir:**
> "Antes de entrenar cualquier modelo, necesitamos entender nuestros datos. Un mal dataset no se arregla con un buen modelo."

### Slide 5: Dataset Iris - Caracter√≠sticas
**Mostrar:**
- 150 muestras, 4 features
- 3 clases balanceadas (50 cada una)
- Sin valores faltantes
- Features num√©ricas ya normalizadas

**C√≥digo clave a mostrar:**
```python
# Cargar y explorar
iris = datasets.load_iris()
df = pd.DataFrame(X, columns=iris.feature_names)
print(f"Shape: {df.shape}")
print(f"Clases: {iris.target_names}")
```

**Qu√© vas a decir:**
> "Esto es exploraci√≥n, no ML. Queremos entender: ¬øqu√© tipo de problema es? ¬øLos datos est√°n limpios? ¬øHay balance en las clases? Esta fase conecta directamente con el data lifecycle que ya conocemos."

### Slide 6: Conexi√≥n con Data Lifecycle
**Mostrar tabla:**
| Data Lifecycle | Lo que hicimos |
|----------------|----------------|
| Ingesti√≥n | `datasets.load_iris()` |
| Calidad | Verificar missing values, tipos |
| Transformaci√≥n | M√≠nima (datos ya limpios) |

**Qu√© vas a decir:**
> "Vean c√≥mo los conceptos del data lifecycle se aplican aqu√≠. La diferencia es que ahora el output no es un dashboard, sino un modelo."

---

## üê≥ FASE 2: ENTORNO REPRODUCIBLE (8 minutos)

### Slide 7: ¬øPor qu√© Docker?
**Problemas que resuelve:**
- "En mi m√°quina funciona"
- Dependencias conflictivas
- Versiones diferentes de Python/librer√≠as
- Dificultad para desplegar

**Qu√© vas a decir:**
> "Docker no es solo para DevOps. En ML, la reproducibilidad es cr√≠tica. Si no puedes reproducir tu experimento, no puedes confiar en √©l."

### Slide 8: Dockerfile Explicado
**Mostrar c√≥digo l√≠nea por l√≠nea:**
```dockerfile
FROM python:3.10-slim          # ¬øPor qu√© slim?
WORKDIR /app                   # Organizaci√≥n
COPY requirements.txt .        # ¬øPor qu√© primero?
RUN pip install -r requirements.txt
COPY src/ src/                 # C√≥digo fuente
COPY data/ data/               # Dataset
EXPOSE 5000                    # Puerto MLflow
CMD ["python", "src/train.py"] # Punto de entrada
```

**Qu√© vas a decir para cada l√≠nea:**
- **slim**: "Imagen m√°s peque√±a, menos superficie de ataque, descarga m√°s r√°pida"
- **requirements primero**: "Aprovecha el cache de Docker. Solo reinstala si cambian dependencias"
- **EXPOSE**: "Documentamos qu√© puerto usa MLflow UI"

### Slide 9: Principio de Dependencias M√≠nimas
**Mostrar requirements.txt:**
```txt
pandas>=1.5.0
scikit-learn>=1.3.0
mlflow>=2.8.0
```

**Qu√© vas a decir:**
> "Solo lo esencial. Menos dependencias = menos problemas. Cada librer√≠a adicional es un punto de falla potencial."

---

## üéØ FASE 3: ENTRENAMIENTO + MLFLOW (12 minutos)

### Slide 10: ¬øQu√© es MLflow?
**Definici√≥n simple:**
> "MLflow = Git + Docker + M√©tricas para Modelos"

**Componentes:**
- **Tracking**: Experimentos, par√°metros, m√©tricas
- **Registry**: Versionado de modelos
- **Serving**: Modelos como APIs

### Slide 11: Anatom√≠a de un MLflow Run
**Mostrar c√≥digo estructura:**
```python
with mlflow.start_run() as run:
    # 1. Registrar par√°metros
    mlflow.log_param("solver", "lbfgs")
    
    # 2. Entrenar modelo
    model = LogisticRegression(**params)
    model.fit(X_train, y_train)
    
    # 3. Registrar m√©tricas
    mlflow.log_metric("test_accuracy", accuracy)
    
    # 4. Registrar modelo
    mlflow.sklearn.log_model(model, "iris_classifier")
```

**Qu√© vas a decir:**
> "Cada run es un experimento completo. MLflow registra autom√°ticamente todo: cu√°ndo corri√≥, qu√© par√°metros us√≥, qu√© m√©tricas obtuvo, y el modelo resultante."

### Slide 12: Conceptos Clave MLflow
**Definir cada uno:**
- **Run**: Experimento individual con ID √∫nico
- **Experiment**: Agrupaci√≥n de runs relacionados
- **Parameters**: Hiperpar√°metros del modelo (input)
- **Metrics**: M√©tricas de evaluaci√≥n (output)
- **Artifacts**: Archivos generados (modelo, gr√°ficos, etc.)

### Slide 13: Resultados Obtenidos
**Mostrar m√©tricas:**
```
‚úÖ Test Accuracy: 96.67%
‚úÖ Train Accuracy: 97.50%
‚úÖ Overfitting: 0.83% (excelente)
‚úÖ F1-Score: 96.66%
```

**Qu√© vas a decir:**
> "Excelentes resultados, pero lo importante no es el 96% de accuracy. Lo importante es que todo est√° registrado y es reproducible. Cualquiera puede tomar este run ID y obtener exactamente el mismo modelo."

### Slide 14: ¬øPor qu√© Tracking es Cr√≠tico?
**Escenarios reales:**
- Comparar 50 experimentos diferentes
- Recordar qu√© funcion√≥ hace 3 meses
- Reproducir resultados para un paper
- Explicar a tu jefe por qu√© el modelo cambi√≥

**Qu√© vas a decir:**
> "Sin tracking, cada experimento es una caja negra. Con MLflow, tienes trazabilidad completa."

---

## üè∑Ô∏è FASE 4: MODEL REGISTRY (8 minutos)

### Slide 15: Modelo ‚â† Archivo
**Comparaci√≥n visual:**
```
‚ùå modelo.pkl
   - Solo pesos
   - Sin contexto
   - Sin versi√≥n
   - Sin m√©tricas

‚úÖ Modelo MLflow
   - C√≥digo + Pesos
   - Entorno completo
   - Versi√≥n autom√°tica
   - M√©tricas + Lineage
```

**Qu√© vas a decir:**
> "Esta es la diferencia fundamental. Un .pkl es solo un archivo. Un modelo registrado en MLflow es un artefacto de software completo."

### Slide 16: Model Registry - Governance
**Stages del modelo:**
- **None**: Reci√©n registrado
- **Staging**: Listo para testing
- **Production**: Sirviendo usuarios reales
- **Archived**: Deprecado

**Qu√© vas a decir:**
> "Esto es governance. No cualquier modelo puede ir a producci√≥n. Debe pasar por staging, ser validado, y solo entonces promovido."

### Slide 17: MLflow UI - Demo
**Mostrar screenshots de:**
- Experiments tab con runs
- Models tab con versiones
- Comparaci√≥n de m√©tricas
- Transici√≥n de stages

**Qu√© vas a decir:**
> "La UI nos da visibilidad completa. Podemos comparar experimentos, ver la evoluci√≥n de m√©tricas, y gestionar el ciclo de vida de modelos."

---

## üåê FASE 5: MODEL SERVING (10 minutos)

### Slide 18: ¬øPor qu√© Serving?
**Qu√© vas a decir:**
> "Un modelo que no se puede consumir es solo un experimento. El valor real est√° en generar predicciones para sistemas en producci√≥n."

### Slide 19: Anatom√≠a de una API de ML
**Mostrar estructura:**
```python
@app.route('/invocations', methods=['POST'])
def predict():
    # Input: {"instances": [[5.1, 3.5, 1.4, 0.2]]}
    # Output: {"predictions": [0]}
```

**Endpoints implementados:**
- `GET /health`: Health check
- `POST /invocations`: Predicciones (formato MLflow)
- `POST /predict_names`: Con nombres de clases
- `GET /`: Info del servicio

### Slide 20: Test de Inferencia
**Mostrar ejemplo real:**
```python
# Datos de prueba
test_samples = [
    [5.1, 3.5, 1.4, 0.2],  # Setosa
    [7.0, 3.2, 4.7, 1.4],  # Versicolor  
    [6.3, 3.3, 6.0, 2.5]   # Virginica
]

# Request
curl -X POST http://localhost:1235/invocations \
  -H 'Content-Type: application/json' \
  -d '{"instances": [[5.1, 3.5, 1.4, 0.2]]}'

# Response
{"predictions": [0]}
```

**Qu√© vas a decir:**
> "As√≠ es como otros sistemas consumir√≠an nuestro modelo. JSON in, JSON out. Est√°ndar, simple, escalable."

---

## üîÑ CONEXI√ìN DATA ‚Üí MODEL LIFECYCLE (8 minutos)

### Slide 21: Mapeo Conceptual
**Tabla comparativa:**
| Data Lifecycle | Model Lifecycle | Qu√© se Mantiene | Qu√© se Ampl√≠a |
|----------------|-----------------|-----------------|---------------|
| Ingesti√≥n | Feature Extraction | Pipelines | Versionado |
| Transformaci√≥n | Training | Reproducibilidad | M√©tricas |
| Consumo | Inference | APIs/Outputs | Serving/Monitoreo |

**Qu√© vas a decir:**
> "Vean c√≥mo los conceptos se mapean. No estamos aprendiendo algo completamente nuevo, estamos extendiendo lo que ya sabemos sobre datos hacia modelos."

### Slide 22: Flujo Completo Implementado
**Diagrama de flujo:**
```
Datos ‚Üí Exploraci√≥n ‚Üí Features ‚Üí Entrenamiento ‚Üí Modelo ‚Üí Registry ‚Üí Serving ‚Üí Predicciones
  ‚Üì         ‚Üì           ‚Üì           ‚Üì           ‚Üì         ‚Üì         ‚Üì          ‚Üì
 CSV     Pandas      Arrays    Scikit-learn  MLflow   Versions   Flask    JSON API
```

**Qu√© vas a decir:**
> "Este es el pipeline completo que implementamos. Cada paso tiene su herramienta, cada herramienta tiene su prop√≥sito."

---

## üöÄ ESCALABILIDAD HACIA PRODUCCI√ìN (6 minutos)

### Slide 23: Taller vs Producci√≥n
**Tabla de mapeo:**
| Taller | Producci√≥n | AWS Service |
|--------|------------|-------------|
| Docker local | Container Registry | ECR |
| MLflow local | Managed ML Platform | SageMaker |
| Flask serving | API Gateway + Compute | API Gateway + Lambda |
| Manual deploy | CI/CD Pipelines | CodePipeline |

**Qu√© vas a decir:**
> "Lo que hicimos hoy es la base. En producci√≥n, cada componente tiene su equivalente escalable y managed."

### Slide 24: Pr√≥ximos Pasos T√©cnicos
**Roadmap:**
1. **CI/CD**: Automatizar entrenamiento y despliegue
2. **Monitoring**: M√©tricas de modelo en producci√≥n
3. **A/B Testing**: Comparar versiones de modelos
4. **Auto-scaling**: Manejar carga variable
5. **Security**: Autenticaci√≥n y autorizaci√≥n

---

## üí° MENSAJES CLAVE Y CIERRE (5 minutos)

### Slide 25: Frases Potentes
**Destacar estos mensajes:**

> **"Un modelo que no est√° trackeado, no existe"**
> Sin tracking, no hay reproducibilidad ni comparaci√≥n.

> **"Un modelo que no se puede consumir, es solo un experimento"**
> El valor est√° en generar predicciones para sistemas reales.

> **"MLflow no es solo para ML, Docker no es solo DevOps"**
> Es software engineering aplicado a modelos.

### Slide 26: Lo que Logramos Hoy
**Checklist visual:**
- ‚úÖ Pipeline completo: Datos ‚Üí Modelo ‚Üí API
- ‚úÖ Reproducibilidad con Docker
- ‚úÖ Tracking completo con MLflow
- ‚úÖ Versionado de modelos
- ‚úÖ Serving como servicio REST
- ‚úÖ Base s√≥lida para MLOps

### Slide 27: Pregunta Final
**"¬øYa no tienen miedo a poner un modelo a correr?"**

**Qu√© vas a decir:**
> "Esa era la meta. Que entiendan que los modelos son software, que se pueden versionar, trackear, y desplegar como cualquier aplicaci√≥n. Ya tienen las herramientas y el conocimiento para hacerlo."

---

## üéØ TIPS PARA LA EXPOSICI√ìN

### Antes de Empezar
1. **Tener MLflow UI abierto** en http://localhost:5000
2. **Preparar terminal** con comandos listos
3. **Screenshots** de cada fase por si algo falla
4. **C√≥digo visible** en editor para mostrar

### Durante la Presentaci√≥n
1. **Mostrar c√≥digo real**, no solo slides
2. **Ejecutar comandos en vivo** cuando sea posible
3. **Conectar cada concepto** con problemas reales
4. **Usar analog√≠as**: "MLflow es como Git para modelos"
5. **Hacer preguntas** para mantener engagement

### Manejo de Preguntas
**Preguntas frecuentes y respuestas:**

**"¬øPor qu√© no usar Jupyter notebooks?"**
> "Notebooks son excelentes para exploraci√≥n, pero no para producci√≥n. No son reproducibles, no versionan bien, y no escalan."

**"¬øMLflow es mejor que Weights & Biases?"**
> "Son herramientas similares. MLflow es open source y se integra bien con cualquier stack. W&B tiene mejor UI pero es SaaS."

**"¬øEsto funciona con deep learning?"**
> "Absolutamente. MLflow soporta TensorFlow, PyTorch, cualquier framework. Los conceptos son los mismos."

**"¬øC√≥mo manejan datos grandes?"**
> "Para datos grandes, usar√≠as S3 + Spark + SageMaker. Los principios son iguales, solo cambia la escala."

### Timing por Secci√≥n
- **Introducci√≥n**: 5 min (no te extiendas)
- **Fase 1-2**: 8 min cada una (mostrar c√≥digo)
- **Fase 3**: 12 min (la m√°s importante)
- **Fase 4-5**: 8-10 min cada una
- **Conexiones**: 8 min (conceptual)
- **Producci√≥n**: 6 min (futuro)
- **Cierre**: 5 min (mensajes clave)

### Backup Plans
1. **Si MLflow UI falla**: Usar screenshots
2. **Si Docker falla**: Mostrar c√≥digo y explicar conceptos
3. **Si serving falla**: Usar curl examples y mostrar JSON
4. **Si todo falla**: Focus en conceptos y arquitectura

---

## üé§ SCRIPT DE APERTURA

**"Buenos d√≠as/tardes. Hoy vamos a implementar juntos un pipeline completo de Machine Learning. No vamos a hacer el modelo m√°s preciso del mundo, vamos a hacer algo m√°s importante: un modelo que puede vivir en producci√≥n.**

**¬øCu√°ntos han tenido un notebook que funciona perfecto, pero no saben c√≥mo ponerlo en producci√≥n? ¬øO han entrenado 20 modelos y no recuerdan cu√°l funcion√≥ mejor?**

**Esos son problemas reales. Y hoy los vamos a resolver con MLflow y Docker. Al final de esta sesi√≥n, van a tener un modelo sirviendo predicciones via API REST, completamente trackeado y reproducible.**

**La regla de oro de hoy: Python local solo para explorar datos. Todo lo importante ocurre en Docker. ¬øListos? Empezamos."**

---

**üéØ ¬°Con esta gu√≠a tienes todo lo necesario para una exposici√≥n exitosa del workshop!**